{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wiki_dump_extractor import WikiAvroDumpExtractor, page_utils\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from utils.db_utils import LMDBReader, LMDBWriter\n",
    "import json\n",
    "from rapidfuzz import process, fuzz\n",
    "from rapidfuzz.process import cdist\n",
    "import itertools\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def score_section(section, event):\n",
    "    small_terms = [\n",
    "        event[\"when\"],\n",
    "        event[\"what\"],\n",
    "        event[\"who\"],\n",
    "        event[\"where\"],\n",
    "    ]  #  event[\"where\"], event[\"city\"]\n",
    "    results = process.extract(\n",
    "        section.text,  # the big string\n",
    "        small_terms,  # list of small strings\n",
    "        scorer=fuzz.partial_ratio,\n",
    "        limit=None,  # return all matches, not just the top 5\n",
    "    )\n",
    "    scores = [score for _, score, _ in results]\n",
    "    return sum(scores)\n",
    "\n",
    "\n",
    "def score_all_sections(section, event):\n",
    "    score = score_section(section, event)\n",
    "    result = {section.title: score}\n",
    "    for sub_section in section.children:\n",
    "        result.update(score_all_sections(sub_section, event))\n",
    "    return result\n",
    "\n",
    "\n",
    "def extract_page_section_words(page_text):\n",
    "    section = page_utils.Section.from_page_text(page_text)\n",
    "    text_by_section = section.all_subsections_text_dict()\n",
    "\n",
    "    sections_and_words = [\n",
    "        (\n",
    "            section_title,\n",
    "            sorted(set(re.findall(r\"\\w+\", section_text.lower()))),\n",
    "        )\n",
    "        for section_title, section_text in text_by_section.items()\n",
    "    ]\n",
    "    sections_titles, sections_words = zip(*sections_and_words)\n",
    "    return sections_titles, sections_words\n",
    "\n",
    "\n",
    "def extract_event_words(event):\n",
    "    event_text = \" \".join([event[\"what\"], event[\"where\"], event[\"who\"], event[\"when\"]])\n",
    "    event_words = sorted(set(re.findall(r\"\\w+\", event_text.lower())))\n",
    "    return event_words\n",
    "\n",
    "\n",
    "def attribute_section_to_events(events, page_text):\n",
    "    sections_titles, sections_words = extract_page_section_words(page_text)\n",
    "    sections_titles = [\n",
    "        section_title\n",
    "        for (section_title, words) in zip(sections_titles, sections_words)\n",
    "        if len(words) > 0\n",
    "    ]\n",
    "    sections_words = [words for words in sections_words if len(words) > 0]\n",
    "    event_words = [extract_event_words(event) for event in events]\n",
    "\n",
    "    all_section_words = sorted(set([w for sw in sections_words for w in sw]))\n",
    "    all_event_words = sorted(set([w for words in event_words for w in words]))\n",
    "    # Create word-to-index mappings to avoid expensive np.isin calls\n",
    "    section_word_to_idx = {word: idx for idx, word in enumerate(all_section_words)}\n",
    "    event_word_to_idx = {word: idx for idx, word in enumerate(all_event_words)}\n",
    "    big_grid = cdist(all_event_words, all_section_words, score_cutoff=75, workers=-1)\n",
    "    # Pre-compute section indices to avoid repeated np.isin calls\n",
    "    section_indices = []\n",
    "    for section_words_list in sections_words:\n",
    "        # Use dictionary lookup instead of np.isin\n",
    "        indices = [\n",
    "            section_word_to_idx[word]\n",
    "            for word in section_words_list\n",
    "            if word in section_word_to_idx\n",
    "        ]\n",
    "        section_indices.append(indices)\n",
    "\n",
    "    # Pre-compute all event word indices once\n",
    "    event_indices = []\n",
    "    for words_list in event_words:\n",
    "        indices = [\n",
    "            event_word_to_idx[word] for word in words_list if word in event_word_to_idx\n",
    "        ]\n",
    "        event_indices.append(indices)\n",
    "\n",
    "    # Create section grids using pre-computed indices\n",
    "    big_grid_by_section = [big_grid[:, indices] for indices in section_indices]\n",
    "\n",
    "    for i, (event, ev_indices) in enumerate(zip(events, event_indices)):\n",
    "        # Use pre-computed event indices\n",
    "        section_grids_for_event = []\n",
    "        for section_grid in big_grid_by_section:\n",
    "            # Use pre-computed indices directly\n",
    "            event_grid = section_grid[ev_indices]\n",
    "            section_grids_for_event.append(event_grid.max(axis=1).T)\n",
    "\n",
    "        section_grids_for_event = np.vstack(section_grids_for_event)\n",
    "        scores = (\n",
    "            section_grids_for_event / np.maximum(1, section_grids_for_event.sum(axis=0))\n",
    "        ).sum(axis=1)\n",
    "        best_score_index = scores.argmax()\n",
    "        event[\"section\"] = sections_titles[best_score_index]\n",
    "\n",
    "\n",
    "def batch_iterator(iterable, batch_size=1000):\n",
    "    \"\"\"\n",
    "    Creates batches from an iterator with specified batch size.\n",
    "\n",
    "    Args:\n",
    "        iterable: The iterator to batch\n",
    "        batch_size: Size of each batch\n",
    "\n",
    "    Returns:\n",
    "        Iterator of batches\n",
    "    \"\"\"\n",
    "    iterator = iter(iterable)\n",
    "    while True:\n",
    "        batch = list(itertools.islice(iterator, batch_size))\n",
    "        if not batch:\n",
    "            break\n",
    "        yield batch\n",
    "\n",
    "\n",
    "def remove_date_outliers(events):\n",
    "    events_years = [event[\"when\"].split(\"/\")[0] for event in events]\n",
    "    events_years = [\n",
    "        -int(year[:-3]) if year.endswith(\"BC\") else int(year) for year in events_years\n",
    "    ]\n",
    "    year_mean, year_std = np.mean(events_years), np.std(events_years)\n",
    "    tol = 3 * year_std\n",
    "    filtered_events = [\n",
    "        event\n",
    "        for (event, year) in zip(events, events_years)\n",
    "        if (year < -10) or (year > 31) or (year_mean - tol < year < year_mean + tol)\n",
    "    ]\n",
    "    n_removed = len(events) - len(filtered_events)\n",
    "    if n_removed > 0:\n",
    "        print(f\"Removed {n_removed} events\")\n",
    "    return filtered_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <generator object LMDBReader.__iter__ at 0x1043a0f40>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/valentin/Documents/programming/landnotes/landnotes-data/utils/db_utils.py\", line 49, in __iter__\n",
      "    with self.db.begin() as txn:\n",
      "lmdb.Error: Attempt to operate on closed/deleted/dropped object.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "743e5c87c7da4b3b8fe1e68448f0a2fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <generator object LMDBReader.__iter__ at 0x1147fab60>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/valentin/Documents/programming/landnotes/landnotes-data/utils/db_utils.py\", line 49, in __iter__\n",
      "lmdb.Error: Attempt to operate on closed/deleted/dropped object.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "3 pages not found in index:first ones are ['$1 Million Challenge', '1 World Trade Center (1971–2001)', '1348 Friuli earthquake']",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     23\u001b[39m batch = \u001b[38;5;28mlist\u001b[39m(batch)\n\u001b[32m     24\u001b[39m page_titles = [title \u001b[38;5;28;01mfor\u001b[39;00m title, _ \u001b[38;5;129;01min\u001b[39;00m batch]\n\u001b[32m     25\u001b[39m page_texts_by_title = {\n\u001b[32m     26\u001b[39m     page.title: page.text\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m page \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdump\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_page_batch_by_title\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpage_titles\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m }\n\u001b[32m     29\u001b[39m results_batch = []\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m page_title, events \u001b[38;5;129;01min\u001b[39;00m batch:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/programming/wiki_dump_extractor/src/wiki_dump_extractor/wiki_dump_extractor.py:581\u001b[39m, in \u001b[36mWikiAvroDumpExtractor.get_page_batch_by_title\u001b[39m\u001b[34m(self, titles, redirects_env, ignore_titles_not_found)\u001b[39m\n\u001b[32m    577\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    578\u001b[39m                 missing_titles = [\n\u001b[32m    579\u001b[39m                     title \u001b[38;5;28;01mfor\u001b[39;00m title, p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(titles, pages) \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    580\u001b[39m                 ]\n\u001b[32m--> \u001b[39m\u001b[32m581\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    582\u001b[39m                     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(missing_titles)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m pages not found in index:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    583\u001b[39m                     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mfirst ones are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing_titles[:\u001b[32m10\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    584\u001b[39m                 )\n\u001b[32m    585\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    586\u001b[39m     env.close()\n",
      "\u001b[31mValueError\u001b[39m: 3 pages not found in index:first ones are ['$1 Million Challenge', '1 World Trade Center (1971–2001)', '1348 Friuli earthquake']"
     ]
    }
   ],
   "source": [
    "generated_data_dir = Path(\"generated_data\")\n",
    "wiki_data_dir = Path(\"wikipedia_data\")\n",
    "dump = WikiAvroDumpExtractor(\n",
    "    wiki_data_dir / \"wiki_dump.avro\", index_dir=wiki_data_dir / \"wiki_dump_index_db\"\n",
    ")\n",
    "target = generated_data_dir / \"events_extracted_by_page_gemini-2.0_processed_db\"\n",
    "\n",
    "if True or not target.exists():\n",
    "    with LMDBWriter(target) as target_db:\n",
    "        with LMDBReader(\n",
    "            generated_data_dir / \"events_extracted_by_page_gemini-2.0-flash_lmdb\"\n",
    "        ) as events_db:\n",
    "            iterator = (\n",
    "                (page, events)\n",
    "                for page, events in events_db\n",
    "                if target_db.get(page.encode()) is None\n",
    "            )\n",
    "            batches = (itertools.islice(iterator, 1000) for _ in itertools.count())\n",
    "            for batch in tqdm(batches):\n",
    "                batch = list(batch)\n",
    "                if len(batch) == 0:\n",
    "                    break\n",
    "                batch = list(batch)\n",
    "                page_titles = [title for title, _ in batch]\n",
    "                page_texts_by_title = {\n",
    "                    page.title: page.text\n",
    "                    for page in dump.get_page_batch_by_title(\n",
    "                        page_titles, ignore_titles_not_found=True\n",
    "                    )\n",
    "                }\n",
    "                results_batch = []\n",
    "                for page_title, events in batch:\n",
    "                    try:\n",
    "                        events = json.loads(events.decode())\n",
    "                    except Exception:\n",
    "                        continue\n",
    "\n",
    "                    page_text = page_texts_by_title[page_title.decode()]\n",
    "                    attribute_section_to_events(events, page_text)\n",
    "                    results_batch.append(\n",
    "                        (page_title.encode(), json.dumps(events).encode())\n",
    "                    )\n",
    "\n",
    "                target_db.write_batch(results_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "485cdcfcbe5a4a9785b396df57a5fa15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
