{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e893ebd9",
   "metadata": {},
   "source": [
    "# Creating a wikipedia database\n",
    "\n",
    "speed is typically 2000 pages per second uncompressed\n",
    "250 pages per second on the compressed bz2 file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "697bac87",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e1442f",
   "metadata": {},
   "source": [
    "## Download the wikipedia dump\n",
    "\n",
    "This downloads the 28Gb Wikipedia dump from the web.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332e015a",
   "metadata": {},
   "source": [
    "Instead of running the following cell which downloads the wikipedia dump from a web url, you can run the following command in the terminal, which downloads it via a torrent. The torrent download is a bit more manual but much faster (the torrent takes 15mins at ~20Mb/s, the web download takes 1-2h at 4Mb/s and is flimsy, subject to cuts)\n",
    "\n",
    "Torrents are found [here](https://meta.wikimedia.org/wiki/Data_dump_torrents)\n",
    "\n",
    "```bash\n",
    "# MacOS: brew install aria2\n",
    "# Linux: sudo apt install aria2\n",
    "aria2c \\\n",
    "    -d wikipedia_data \\\n",
    "    --seed-time=0\n",
    "    https://academictorrents.com/download/cd872797612d95384de3a0ab7e6a1f156bf91495.torrent\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d75e5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wikipedia_data/enwiki-20250501-pages-articles-multistream.xml.bz2 already exists, skipping download.\n"
     ]
    }
   ],
   "source": [
    "# You can safely run this cell even if you have already downloaded the wikipedia dump\n",
    "# it will simply skip the download if the file already exists\n",
    "\n",
    "from wiki_dump_extractor import WikiXmlDumpExtractor, download_file\n",
    "from pathlib import Path\n",
    "\n",
    "dump_date = \"20250501\"\n",
    "http_wiki_dump_dir = f\"https://dumps.wikimedia.org/enwiki/{dump_date}/\"\n",
    "wiki_dump_name = f\"enwiki-{dump_date}-pages-articles-multistream.xml.bz2\"\n",
    "dump_url = http_wiki_dump_dir + wiki_dump_name\n",
    "wikipedia_data_dir = Path(\"wikipedia_data\")\n",
    "local_dump_path = wikipedia_data_dir / wiki_dump_name\n",
    "download_file(dump_url, local_dump_path, replace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3ae431",
   "metadata": {},
   "source": [
    "## Extract the dump to an Avro file\n",
    "\n",
    "This creates a more practical version of the archive, which is 28Gb, but faster to iterate through (10-20x) and easier to fetch by page title.\n",
    "\n",
    "This will go over 24 million compressed pages (2400 batches of 10000 pages). This operation takes ~40mins on a good processor, up to ~2-3h on an older one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b99a679f",
   "metadata": {},
   "outputs": [],
   "source": [
    "avro_dump_path = wikipedia_data_dir / \"wiki_dump.avro\"\n",
    "page_index_db = wikipedia_data_dir / \"wiki_dump_index_db\"\n",
    "redirects_db = wikipedia_data_dir / \"wiki_dump_redirects_db\"\n",
    "\n",
    "\n",
    "if not avro_dump_path.exists():\n",
    "    extractor = WikiXmlDumpExtractor(file_path=local_dump_path)\n",
    "    ignored_fields = [\"timestamp\", \"page_id\", \"revision_id\", \"redirect_title\"]\n",
    "    extractor.extract_pages_to_avro(\n",
    "        output_file=avro_dump_path,\n",
    "        redirects_db_path=redirects_db,\n",
    "        batch_size=10_000,\n",
    "        ignored_fields=ignored_fields,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1dbf8dd",
   "metadata": {},
   "source": [
    "## Index the pages\n",
    "\n",
    "This creates an index so that pages can be fetched by title in the future. This takes ~4mins on a good processor, up to ~3x on an older one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc8540be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wiki_dump_extractor import WikiAvroDumpExtractor\n",
    "\n",
    "if not page_index_db.exists():\n",
    "    dump = WikiAvroDumpExtractor(avro_dump_path)\n",
    "    dump.index_pages(index_dir=page_index_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa836d6",
   "metadata": {},
   "source": [
    "## Extract the titles of disambiguation pages\n",
    "\n",
    "This finds the titles of pages which are disambiguation pages. For instance \"Marie Louise\" is a valid page title but the page doesn't refer to a particular person, but rather to a category of people named \"Marie Louise\".\n",
    "\n",
    "This processes 12 million pages and takes 12mins on a good processor, probably 3 times that on an older one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f984961d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wiki_dump_extractor import WikiAvroDumpExtractor\n",
    "\n",
    "disamsbiguation_page_titles_path = (\n",
    "    wikipedia_data_dir / \"disambiguation_page_titles.json\"\n",
    ")\n",
    "if not disamsbiguation_page_titles_path.exists():\n",
    "    dump = WikiAvroDumpExtractor(avro_dump_path)\n",
    "    dump.extract_disambiguation_page_titles(disamsbiguation_page_titles_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cf9e7e",
   "metadata": {},
   "source": [
    "## Extract links from the pages\n",
    "\n",
    "This extracts the links from the pages. Every tisme there is a link like `[[Bombay | Mumbai]]` it records an entry associating the shown text (Bombay) to the wikipedia page (Mumbai) which helps create a \"dictionary of synonyms\". Some of these are specific to the page in which they are. For instance you might find `[[Marie-Therese | Infanta Maria Theresa of Portugal]]` in one page, and then `[[Maria Theresa | Maria Theresa of Spain ]]` in another.\n",
    "\n",
    "This processes 12 million pages (1200 batches of 10,000 pages)and takes 10mins on a good processor with 6 workers, probably 3 times that on an older one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92d41e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import db_utils\n",
    "from utils.extraction_utils import find_links_in_pages\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "from wiki_dump_extractor import WikiAvroDumpExtractor\n",
    "\n",
    "wiki_data_dir = Path(\"wikipedia_data\")\n",
    "dump = WikiAvroDumpExtractor(\n",
    "    wiki_data_dir / \"wiki_dump.avro\", index_dir=wiki_data_dir / \"wiki_dump_idx\"\n",
    ")\n",
    "\n",
    "generated_data_dir = Path(\"generated_data\")\n",
    "page_links_db = generated_data_dir / \"page_links_db\"\n",
    "if not page_links_db.exists():\n",
    "    processed_batches = dump.process_page_batches_in_parallel(\n",
    "        process_fn=find_links_in_pages, batch_size=10_000, num_workers=6\n",
    "    )\n",
    "    with db_utils.LMDBWriter(page_links_db, map_size=20_000_000_000) as db:\n",
    "        for batch_result in tqdm(processed_batches):\n",
    "            db.write_batch(batch_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939dd884",
   "metadata": {},
   "source": [
    "## Extract infobox data\n",
    "\n",
    "This extracts the infoboxes from the pages.\n",
    "\n",
    "This processes 12 million pages (1200 batches of 10,000 pages) and takes 7mins on a good processor with 6 workers, probably 3 times that on an older one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34c08dca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e26299059744a3c813b2537adf8a2a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4392949\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from utils import db_utils, extraction_utils\n",
    "\n",
    "generated_data_dir = Path(\"generated_data\")\n",
    "parsed_infoboxes_db = generated_data_dir / \"parsed_infoboxes_db\"\n",
    "if not parsed_infoboxes_db.exists():\n",
    "    counter = 0\n",
    "    processed_batches = dump.process_page_batches_in_parallel(\n",
    "        process_fn=extraction_utils.parse_infoboxes,\n",
    "        batch_size=10_000,\n",
    "        num_workers=6,\n",
    "    )\n",
    "    with db_utils.LMDBWriter(parsed_infoboxes_db, map_size=20_000_000_000) as db:\n",
    "        for batch_result in tqdm(processed_batches):\n",
    "            counter += len(batch_result)\n",
    "            db.write_batch(batch_result)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc506778",
   "metadata": {},
   "source": [
    "## Extract pages linked in the \"Year\" pages\n",
    "\n",
    "This goes through wikipedia's \"year pages\" such as [1808 (year)](<https://en.wikipedia.org/wiki/1808_(year)>) and extracts all the pages linked in them, which include notable events, notable places and notable people's birth and death. These are great first candidates for AI extraction of events. The process takes a couple minutes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbe8b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "wiki_data_dir = Path(\"wikipedia_data\")\n",
    "link_regex = re.compile(r\"\\[\\[([^|\\]]+)(?:\\|[^\\]]*)?]]\")\n",
    "months = \"January|February|March|April|May|June|July|August|September|October|November|December\"\n",
    "month_day_regex = re.compile(f\"^({months}) \\\\d+$\")\n",
    "day_month_regex = re.compile(f\"^\\\\d{{1,2}} ({months})$\")\n",
    "year_regex = re.compile(r\"^\\d{1,4}$\")\n",
    "\n",
    "\n",
    "def extract_links(page_text):\n",
    "    return [\n",
    "        link.strip()\n",
    "        for link in list(set(link_regex.findall(page_text)))\n",
    "        if not month_day_regex.match(link)\n",
    "        and not day_month_regex.match(link)\n",
    "        and not year_regex.match(link)\n",
    "        and not link.startswith(\"Category:\")\n",
    "        and not link.startswith(\"File:\")\n",
    "    ]\n",
    "\n",
    "\n",
    "def year_to_page_name(year, redirects_db):\n",
    "    if year < 0:\n",
    "        page_name = f\"{-year} BC\"\n",
    "    else:\n",
    "        page_name = f\"{year} (year)\"\n",
    "    redirect = db_utils.get_redirect(title=page_name, db=redirects_db)\n",
    "    if redirect:\n",
    "        return redirect\n",
    "    return page_name\n",
    "\n",
    "\n",
    "target = wiki_data_dir / \"historical_pages.avro\"\n",
    "if not target.exists():\n",
    "    with db_utils.LMDBReader(wiki_data_dir / \"wiki_dump_redirects_db\") as redirects_env:\n",
    "        page_names = [\n",
    "            year_to_page_name(year, redirects_env) for year in range(-500, 2000)\n",
    "        ]\n",
    "        pages = dump.get_page_batch_by_title(page_names)\n",
    "        all_page_links = [link for page in pages for link in extract_links(page.text)]\n",
    "        all_page_links = sorted(set(all_page_links))\n",
    "        dump.extract_pages_titles_to_new_dump(\n",
    "            all_page_links,\n",
    "            target,\n",
    "            redirects_env=redirects_env,\n",
    "            ignore_titles_not_found=True,\n",
    "        )\n",
    "    historical_dump = WikiAvroDumpExtractor(target)\n",
    "    historical_dump.index_pages(wiki_data_dir / \"historical_pages_idx\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
