{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "faffc96d",
   "metadata": {},
   "source": [
    "# Extract events (births, deaths, openings, releases...) from infoboxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53a38be7-2726-4909-861c-847bde12ca60",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-14T11:51:46.219934Z",
     "iopub.status.busy": "2025-03-14T11:51:46.219033Z",
     "iopub.status.idle": "2025-03-14T11:51:46.255412Z",
     "shell.execute_reply": "2025-03-14T11:51:46.254144Z",
     "shell.execute_reply.started": "2025-03-14T11:51:46.219875Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d2099d",
   "metadata": {},
   "source": [
    "## Gather all infoboxes with at least one date and one location\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4131c35c-dee1-4863-af05-97b4a3d25ccb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-14T11:51:54.989250Z",
     "iopub.status.busy": "2025-03-14T11:51:54.988836Z",
     "iopub.status.idle": "2025-03-14T11:51:55.668550Z",
     "shell.execute_reply": "2025-03-14T11:51:55.667035Z",
     "shell.execute_reply.started": "2025-03-14T11:51:54.989217Z"
    }
   },
   "outputs": [],
   "source": [
    "from wiki_dump_extractor import page_utils\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "import json\n",
    "from utils import db_utils\n",
    "import zlib\n",
    "\n",
    "wiki_data_dir = Path(\"wikipedia_data/\")\n",
    "generated_data_dir = Path(\"generated_data/\")\n",
    "\n",
    "date_keywords = [\n",
    "    \"date\",\n",
    "    \"released\",\n",
    "    \"published\",\n",
    "    \"founded\",\n",
    "    \"established\",\n",
    "    \"_start\",\n",
    "    \"_end\",\n",
    "    \"year\",\n",
    "    \"build\",\n",
    "    \"opened\",\n",
    "]\n",
    "\n",
    "place_keywords = [\"place\", \"location\", \"city\", \"coordinates\", \"released\"]\n",
    "output_dir = Path(\"infoboxes/\")\n",
    "\n",
    "\n",
    "def process_infobox(page_title, data):\n",
    "    data[\"page_title\"] = page_title\n",
    "    keys = set(data.keys())\n",
    "\n",
    "    has_date = any([word in k for k in keys for word in date_keywords])\n",
    "    has_location = any(\n",
    "        [(word in k) for k in keys if (\"replace\" not in k) for word in place_keywords]\n",
    "    )\n",
    "    if has_date and has_location:\n",
    "        return data\n",
    "\n",
    "\n",
    "infoboxes_with_date_and_place_db = (\n",
    "    generated_data_dir / \"infoboxes_with_date_and_place_db\"\n",
    ")\n",
    "parsed_infoboxes_db = generated_data_dir / \"parsed_infoboxes_db\"\n",
    "if not infoboxes_with_date_and_place_db.exists():\n",
    "    counter = 0\n",
    "    with db_utils.LMDBWriter(infoboxes_with_date_and_place_db) as db:\n",
    "        with db_utils.LMDBReader(parsed_infoboxes_db) as infoboxes_db:\n",
    "            selected_infoboxes = []\n",
    "            for page_title, zipped_infobox_data in tqdm(infoboxes_db):\n",
    "                decompressed = zlib.decompress(zipped_infobox_data)\n",
    "                infobox_data = json.loads(decompressed.decode())\n",
    "                processed_data = process_infobox(page_title, infobox_data)\n",
    "                compressed = zlib.compress(json.dumps(processed_data).encode())\n",
    "                if processed_data is not None:\n",
    "                    selected_infoboxes.append((page_title.encode(), compressed))\n",
    "                    counter += 1\n",
    "            db.write_batch(selected_infoboxes)\n",
    "    print(len(selected_infoboxes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e496d7",
   "metadata": {},
   "source": [
    "## Make a list of wikipedia people\n",
    "\n",
    "(pages whose infobox has a birth date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e1bccbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "people_pages_json = generated_data_dir / \"people_pages_db,json\"\n",
    "redirects_db_path = wiki_data_dir / \"wiki_dump_redirects_db\"\n",
    "if not people_pages_json.exists():\n",
    "    counters = {\n",
    "        \"birth_event_possible\": 0,\n",
    "        \"death_event_possible\": 0,\n",
    "        \"event_page\": 0,\n",
    "        \"building_page\": 0,\n",
    "    }\n",
    "    people_pages = set()\n",
    "    with db_utils.LMDBReader(infoboxes_with_date_and_place_db) as db:\n",
    "        for key, value in tqdm(db):\n",
    "            decompressed = zlib.decompress(value)\n",
    "            data = json.loads(decompressed.decode())\n",
    "            if \"birth_date\" in data and \"birth_place\" in data:\n",
    "                counters[\"birth_event_possible\"] += 1\n",
    "                people_pages.add(key)\n",
    "            if \"death_date\" in data and \"death_place\" in data:\n",
    "                counters[\"death_event_possible\"] += 1\n",
    "                people_pages.add(key)\n",
    "            if \"date\" in data and (\"location\" in data or \"coordinates\" in data):\n",
    "                counters[\"event_page\"] += 1\n",
    "            if (\"established_date\" in data) or (\"built_date\" in data):\n",
    "                counters[\"building_page\"] += 1\n",
    "    n_redirects = 0\n",
    "    with db_utils.LMDBReader(redirects_db_path) as redirects_db:\n",
    "        for key, value in tqdm(redirects_db):\n",
    "            value = value.decode()\n",
    "            if value in people_pages:\n",
    "                n_redirects += 1\n",
    "                people_pages.add(key)\n",
    "    print(counters)\n",
    "    print(\"Detected people pages:\", len(people_pages))\n",
    "    with people_pages_json.open(\"w\") as f:\n",
    "        json.dump(list(people_pages), f)\n",
    "\n",
    "people_pages = set(json.loads(people_pages_json.read_text()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9f5ac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29d62e37d3bf4982b132a77a220a7edb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "Won't implicitly convert Unicode to bytes; use .encode()",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 216\u001b[39m\n\u001b[32m    214\u001b[39m         batch.append((page_title, json.dumps(records).encode()))\n\u001b[32m    215\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(batch) > \u001b[32m1000\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m             \u001b[43mevents_db\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m             batch = []\n\u001b[32m    218\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batch:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/programming/landnotes/landnotes-data/utils/db_utils.py:72\u001b[39m, in \u001b[36mLMDBWriter.write_batch\u001b[39m\u001b[34m(self, records)\u001b[39m\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.db.begin(write=\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m txn:\n\u001b[32m     71\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m records:\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m         \u001b[43mtxn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: Won't implicitly convert Unicode to bytes; use .encode()"
     ]
    }
   ],
   "source": [
    "from wiki_dump_extractor import date_utils\n",
    "\n",
    "\n",
    "def extract_date_from_infobox_value(value):\n",
    "    value = page_utils.remove_comments_and_citations(value)\n",
    "    parsed_dates, errors = date_utils.extract_dates(value)\n",
    "    dedup_dict = {(d.date.year, d.date.month, d.date.day): d for d in parsed_dates}\n",
    "    dedup_dates = list(dedup_dict.values())\n",
    "    if len(dedup_dates) == 0:\n",
    "        return\n",
    "\n",
    "    full_formats = [\n",
    "        \"WIKI_BIRTH_DATE\",\n",
    "        \"DASH_YMD\",\n",
    "        \"SLASH_DMY_MDY\",\n",
    "        \"MONTH_DAY_YEAR\",\n",
    "    ]\n",
    "    dedup_dates = [d for d in dedup_dates if d.format in full_formats]\n",
    "    if len(dedup_dates) != 1:\n",
    "        return\n",
    "    return dedup_dates[0].date\n",
    "\n",
    "\n",
    "def extract_locations_from_infobox_value(value, locations_by_page_title_db):\n",
    "    value = page_utils.remove_comments_and_citations(value)\n",
    "    if value.strip() == \"\":\n",
    "        return []\n",
    "    locations = []\n",
    "    links = page_utils.extract_links(value)\n",
    "    for link in links.values():\n",
    "        if isinstance(link, str):\n",
    "            maybe_location = locations_by_page_title_db.get(link.encode())\n",
    "            if maybe_location is not None:\n",
    "                locations.append(json.loads(maybe_location.decode()))\n",
    "    if len(locations) == 0:\n",
    "        values = value.split(\",\")\n",
    "        for value in (\",\".join(values[:-i]) for i in range(len(values))):\n",
    "            if value.strip() == \"\":\n",
    "                continue\n",
    "\n",
    "            maybe_location = locations_by_page_title_db.get(value.encode())\n",
    "            if maybe_location is not None:\n",
    "                maybe_location = json.loads(maybe_location.decode())\n",
    "                locations.append(maybe_location)\n",
    "                break\n",
    "    return locations\n",
    "\n",
    "\n",
    "def extract_people_from_infobox_value(value):\n",
    "    value = page_utils.remove_comments_and_citations(value)\n",
    "    if value.strip() == \"\":\n",
    "        return []\n",
    "    people = []\n",
    "    links = page_utils.extract_links(value)\n",
    "    for link in links.values():\n",
    "        if isinstance(link, str):\n",
    "            link = [link]\n",
    "            for mylink in link:\n",
    "                if mylink in people_pages:\n",
    "                    people.append(mylink)\n",
    "    return people\n",
    "\n",
    "\n",
    "def find_historical_events(data, locations_by_page_title_db, page_counter):\n",
    "    location = data.get(\"location\", None)\n",
    "    date = data.get(\"date\", None)\n",
    "    if (location is None) or (date is None):\n",
    "        return []\n",
    "    places = extract_locations_from_infobox_value(\n",
    "        location,\n",
    "        locations_by_page_title_db=locations_by_page_title_db,\n",
    "    )\n",
    "    date = extract_date_from_infobox_value(date)\n",
    "    if (places == []) or (date is None):\n",
    "        return []\n",
    "    records = []\n",
    "    prefix = data[\"page_title\"].replace(\" \", \"_\")\n",
    "    page_counter[\"count\"] += 1\n",
    "    event_id = f\"{prefix}_infobox_0\"\n",
    "    record = {\n",
    "        \"page_title\": data[\"page_title\"],\n",
    "        \"event_id\": event_id,\n",
    "        \"event_type\": \"historical_event\",\n",
    "        \"date\": date.to_string(),\n",
    "        \"place\": places,\n",
    "        \"event_category\": \"historical_page\",\n",
    "    }\n",
    "    records.append(record)\n",
    "    return records\n",
    "\n",
    "\n",
    "def find_place_events(data, locations_by_page_title_db, page_counter):\n",
    "    place_data = locations_by_page_title_db.get(data[\"page_title\"].encode())\n",
    "    if place_data is None:\n",
    "        return []\n",
    "    place_data = json.loads(place_data.decode())\n",
    "    records = []\n",
    "    ignored = [\n",
    "        \"image\",\n",
    "        \"access\",\n",
    "        \"url\",\n",
    "        \"coordinates\",\n",
    "        \"archivedate\",\n",
    "        \"address\",\n",
    "    ]\n",
    "    for key, value in data.items():\n",
    "        if any([word in key.lower() for word in ignored]):\n",
    "            continue\n",
    "        date = extract_date_from_infobox_value(value)\n",
    "        if date is not None:\n",
    "            page_counter[\"count\"] += 1\n",
    "            prefix = data[\"page_title\"].replace(\" \", \"_\")\n",
    "            event_id = f\"{prefix}_infobox_{page_counter['count']:03d}\"\n",
    "            record = {\n",
    "                \"page_title\": data[\"page_title\"],\n",
    "                \"event_id\": event_id,\n",
    "                \"event_type\": key.replace(\"_\", \" \"),\n",
    "                \"date\": date.to_string(),\n",
    "                \"place\": [place_data],\n",
    "                \"event_category\": \"place_page\",\n",
    "                \"people\": [],\n",
    "            }\n",
    "            records.append(record)\n",
    "\n",
    "    return records\n",
    "\n",
    "\n",
    "def find_date_and_place_type_events(data, locations_by_page_title_db, page_counter):\n",
    "    datefields = [s.replace(\"_date\", \"\") for s in data.keys() if s.endswith(\"_date\")]\n",
    "    placefields = [s.replace(\"_place\", \"\") for s in data.keys() if s.endswith(\"_place\")]\n",
    "    intersection = set(datefields).intersection(set(placefields))\n",
    "    records = []\n",
    "\n",
    "    for event in intersection:\n",
    "        date = extract_date_from_infobox_value(data[event + \"_date\"])\n",
    "        if date is None:\n",
    "            continue\n",
    "        places = extract_locations_from_infobox_value(\n",
    "            data[event + \"_place\"],\n",
    "            locations_by_page_title_db=locations_by_page_title_db,\n",
    "        )\n",
    "        if len(places) == 0:\n",
    "            continue\n",
    "        prefix = data[\"page_title\"].replace(\" \", \"_\")\n",
    "        page_counter[\"count\"] += 1\n",
    "        event_id = f\"{prefix}_infobox_{page_counter['count']:03d}\"\n",
    "        record = {\n",
    "            \"page_title\": data[\"page_title\"],\n",
    "            \"event_id\": event_id,\n",
    "            \"event_type\": event.replace(\"_\", \" \"),\n",
    "            \"date\": date.to_string(),\n",
    "            \"place\": places[\n",
    "                0\n",
    "            ],  # too often, the second place is the state, country, etc.\n",
    "            \"event_category\": \"date_and_place\",\n",
    "        }\n",
    "        records.append(record)\n",
    "    return records\n",
    "\n",
    "\n",
    "events_extracted_from_infobox_db = (\n",
    "    generated_data_dir / \"events_extracted_from_infoboxes_db\"\n",
    ")\n",
    "if not events_extracted_from_infobox_db.exists():\n",
    "    counters = {\n",
    "        \"pages_with_infobox_events\": 0,\n",
    "        \"n_events\": 0,\n",
    "    }\n",
    "    with db_utils.LMDBWriter(events_extracted_from_infobox_db) as events_db:\n",
    "        with db_utils.LMDBReader(\n",
    "            generated_data_dir / \"locations_by_page_title_db\"\n",
    "        ) as locations_by_page_title_db:\n",
    "            with db_utils.LMDBReader(infoboxes_with_date_and_place_db) as infoboxes_db:\n",
    "                batch = []\n",
    "                for page_title, zipped_page_data in tqdm(infoboxes_db):\n",
    "                    event_number = 0\n",
    "                    data = json.loads(zlib.decompress(zipped_page_data).decode())\n",
    "\n",
    "                    page_counter = {\"count\": 0}\n",
    "                    records = []\n",
    "                    date_place_events = find_date_and_place_type_events(\n",
    "                        data, locations_by_page_title_db, page_counter\n",
    "                    )\n",
    "                    if date_place_events and (page_title in people_pages):\n",
    "                        for record in date_place_events:\n",
    "                            record[\"people\"] = [page_title]\n",
    "                    records += date_place_events\n",
    "\n",
    "                    place_events = find_place_events(\n",
    "                        data, locations_by_page_title_db, page_counter\n",
    "                    )\n",
    "                    records += place_events\n",
    "\n",
    "                    historical_events = find_historical_events(\n",
    "                        data,\n",
    "                        locations_by_page_title_db,\n",
    "                        page_counter,\n",
    "                    )\n",
    "                    if historical_events:\n",
    "                        people_in_infobox = list(\n",
    "                            set(\n",
    "                                p\n",
    "                                for val in data.values()\n",
    "                                for p in extract_people_from_infobox_value(val)\n",
    "                            )\n",
    "                        )\n",
    "                        for event in historical_events:\n",
    "                            event[\"people\"] = people_in_infobox\n",
    "                        records += historical_events\n",
    "\n",
    "                    if records:\n",
    "                        counters[\"pages_with_infobox_events\"] += 1\n",
    "                        counters[\"n_events\"] += len(records)\n",
    "                        batch.append(\n",
    "                            (page_title.encode(), json.dumps(records).encode())\n",
    "                        )\n",
    "                        if len(batch) > 1000:\n",
    "                            events_db.write_batch(batch)\n",
    "                            batch = []\n",
    "                if batch:\n",
    "                    events_db.write_batch(batch)\n",
    "    counters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de8be2d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pages_with_infobox_events': 961707, 'n_events': 1245583}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c82a312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7c41a8737cc4a5496e2b2aea988cd52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'category': {'date_and_place': 943255,\n",
       "  'place_page': 87811,\n",
       "  'historical_page': 6510},\n",
       " 'all_pages_with_infobox': 948630,\n",
       " 'nonempty_pages': 948630,\n",
       " 'nonempty_not_in_llm_database': 830659}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from db_utils import open_plyvel_db\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "counts = {\"category\": {}}\n",
    "all_records = []\n",
    "with open_plyvel_db(\n",
    "    \"events_extracted_by_page_gemini-2.0-flash_db\", replace=False\n",
    ") as llm_events_db:\n",
    "    with open_plyvel_db(\n",
    "        \"events_extracted_from_infobox_db\", replace=False\n",
    "    ) as infobox_events_db:\n",
    "        for key, value in tqdm(infobox_events_db.iterator()):\n",
    "            counts[\"all_pages_with_infobox\"] = (\n",
    "                counts.get(\"all_pages_with_infobox\", 0) + 1\n",
    "            )\n",
    "            data = json.loads(value.decode())\n",
    "            if len(data) == 0:\n",
    "                continue\n",
    "            counts[\"nonempty_pages\"] = counts.get(\"nonempty_pages\", 0) + 1\n",
    "\n",
    "            if llm_events_db.get(key) is not None:\n",
    "                continue\n",
    "            counts[\"nonempty_not_in_llm_database\"] = (\n",
    "                counts.get(\"nonempty_not_in_llm_database\", 0) + 1\n",
    "            )\n",
    "\n",
    "            for record in data:\n",
    "                counts[\"category\"][record[\"event_category\"]] = (\n",
    "                    counts[\"category\"].get(record[\"event_category\"], 0) + 1\n",
    "                )\n",
    "                all_records.append(record)\n",
    "counts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
